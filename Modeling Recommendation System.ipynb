{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ploting graphs \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly._subplots import make_subplots\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# outliers\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# normalisation and  transform non-num labels  to num for LabelEncoder.\n",
    "from sklearn.preprocessing import QuantileTransformer, LabelEncoder, RobustScaler, StandardScaler,PolynomialFeatures, MinMaxScaler\n",
    "\n",
    "# spliting the data \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# linear regression\n",
    "from sklearn. linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Logistic regression \n",
    "from sklearn. linear_model import LogisticRegression\n",
    "# mesure the performance of classification \n",
    "from sklearn import metrics\n",
    "# Classification models \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# evaluating the prediction \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "# Unsup\n",
    "# Agglomerative \n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "# Dendogram\n",
    "import scipy.cluster.hierarchy as shc\n",
    "# K_Means \n",
    "from sklearn .cluster import KMeans\n",
    "# Yellowbrick \n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# streamlit\n",
    "import streamlit as st\n",
    "# pandas profiling \n",
    "from ydata_profiling import ProfileReport \n",
    "\n",
    "# Artificial network \n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Deep learning \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Chatbot \n",
    "# from nltk.corpus import stopwords\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# spotipy\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import cdist\n",
    "import json\n",
    "import os\n",
    "\n",
    "# saving the model \n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv('data.csv')\n",
    "df_art= pd.read_csv('data_by_artist.csv')\n",
    "df_gen= pd.read_csv('data_by_genres.csv')\n",
    "df_year= pd.read_csv('data_by_year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your credentials are stored as environment variables\n",
    "SPOTIPY_CLIENT_ID = (\"8c662d4fe4f049c9a479cc16d2f05e76\")\n",
    "SPOTIPY_CLIENT_SECRET =(\"d4450a692f7f4263888dc66f21cf8d43\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up authentication for the Spotify API using Client Credentials flow\n",
    "client_credentials_manager= SpotifyClientCredentials(client_id=SPOTIPY_CLIENT_ID, client_secret=SPOTIPY_CLIENT_SECRET)\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of numerical columns to consider for similarity calculations\n",
    "number_cols = ['valence', 'year', 'acousticness', 'danceability', 'duration_ms', 'energy', 'explicit', 'year',\n",
    "               'instrumentalness', 'key', 'liveness', 'loudness', 'mode', 'popularity', 'speechiness', 'tempo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve song data for a given song name\n",
    "def get_song_data(name, data):\n",
    "    try:\n",
    "        return data[data['name'].str.lower() == name].iloc[0]\n",
    "        return song_data\n",
    "    except IndexError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the mean vector of a list of songs\n",
    "def get_mean_vector(song_list, data):\n",
    "    song_vectors = []\n",
    "    for song in song_list:\n",
    "        song_data = get_song_data(song['name'], data)\n",
    "        if song_data is None:\n",
    "            print('Warning: {} does not exist in the dataset'.format(song['name']))\n",
    "            return None\n",
    "        song_vector = song_data[number_cols].values\n",
    "        song_vectors.append(song_vector)\n",
    "    song_matrix = np.array(list(song_vectors))\n",
    "    return np.mean(song_matrix, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to flatten a list of dictionaries into a single dictionary\n",
    "def flatten_dict_list(dict_list):\n",
    "    flattened_dict = defaultdict()\n",
    "    for key in dict_list[0].keys():\n",
    "        flattened_dict[key] = []\n",
    "    for dictionary in dict_list:\n",
    "        for key, value in dictionary.items():\n",
    "            flattened_dict[key].append(value)\n",
    "    return flattened_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the song data using Min-Max Scaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "normalized_data = min_max_scaler.fit_transform(data[number_cols])\n",
    "\n",
    "# Standardize the normalized data using Standard Scaler\n",
    "standard_scaler = StandardScaler()\n",
    "scaled_normalized_data = standard_scaler.fit_transform(normalized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recommend songs based on a list of seed songs\n",
    "def recommend_songs(seed_songs, data, n_recommendations=10):\n",
    "    metadata_cols = ['name', 'artists', 'year']\n",
    "    song_center = get_mean_vector(seed_songs, data)\n",
    "    \n",
    "    # Return an empty list if song_center is missing\n",
    "    if song_center is None:\n",
    "        return []\n",
    "    \n",
    "    # Normalize the song center\n",
    "    normalized_song_center = min_max_scaler.transform([song_center])\n",
    "    \n",
    "    # Standardize the normalized song center\n",
    "    scaled_normalized_song_center = standard_scaler.transform(normalized_song_center)\n",
    "    \n",
    "    # Calculate Euclidean distances and get recommendations\n",
    "    distances = cdist(scaled_normalized_song_center, scaled_normalized_data, 'euclidean')\n",
    "    index = np.argsort(distances)[0]\n",
    "    \n",
    "    # Filter out seed songs and duplicates, then get the top n_recommendations\n",
    "    rec_songs = []\n",
    "    for i in index:\n",
    "        song_name = data.iloc[i]['name']\n",
    "        if song_name not in [song['name'] for song in seed_songs] and song_name not in [song['name'] for song in rec_songs]:\n",
    "            rec_songs.append(data.iloc[i])\n",
    "            if len(rec_songs) == n_recommendations:\n",
    "                break\n",
    "    \n",
    "    return pd.DataFrame(rec_songs)[metadata_cols].to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Infinity by ['One Direction'] (2015)\n",
      "2. Secrets by ['OneRepublic'] (2009)\n",
      "3. In My Blood by ['Shawn Mendes'] (2018)\n",
      "4. Head Above Water by ['Avril Lavigne'] (2019)\n",
      "5. Green Light by ['Lorde'] (2017)\n",
      "6. My Wish by ['Rascal Flatts'] (2006)\n",
      "7. Magic Shop by ['BTS'] (2018)\n",
      "8. Good Things Fall Apart (with Jon Bellion) by ['ILLENIUM', 'Jon Bellion'] (2019)\n",
      "9. Inside Out (feat. Griff) by ['Zedd', 'Griff'] (2020)\n",
      "10. A.M. by ['One Direction'] (2015)\n",
      "11. Love You Goodbye by ['One Direction'] (2015)\n",
      "12. Story of My Life by ['One Direction'] (2013)\n",
      "13. Perfect by ['Simple Plan'] (2018)\n",
      "14. arms by ['Christina Perri'] (2011)\n",
      "15. Breezeblocks by ['alt-J'] (2012)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# List of seed songs (replace with your own seed songs)\n",
    "seed_songs = [\n",
    "    {'name': 'Paranoid'},\n",
    "    {'name': 'Blinding Lights'},\n",
    "    # Add more seed songs as needed\n",
    "]\n",
    "seed_songs = [{'name': name['name'].lower()} for name in seed_songs]\n",
    "\n",
    "# Number of recommended songs\n",
    "n_recommendations = 15\n",
    "\n",
    "# Call the recommend_songs function\n",
    "recommended_songs = recommend_songs(seed_songs, data, n_recommendations)\n",
    "\n",
    "# Convert the recommended songs to a DataFrame\n",
    "recommended_df = pd.DataFrame(recommended_songs)\n",
    "\n",
    "# Print the recommended songs\n",
    "for idx, song in enumerate(recommended_songs, start=1):\n",
    "    print(f\"{idx}. {song['name']} by {song['artists']} ({song['year']})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
